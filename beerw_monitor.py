import requests
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime, date

# ä» GitHub ç¯å¢ƒå˜é‡è¯»å–ä¼ä¸šå¾®ä¿¡Webhook
WEBHOOK_URL = os.getenv("WECOM_WEBHOOK")
# æŒ‡å®šç›‘æ§çš„è¡Œä¸šèµ„è®¯æ ç›®ï¼ˆä½ æä¾›çš„åœ°å€ï¼‰
TARGET_URL = "https://www.beerw.com/class.asp?id=11"
# ç›‘æ§å…³é”®è¯ï¼ˆå¯æŒ‰éœ€å¢å‡ï¼‰
KEYWORDS = ["é’å²›å•¤é…’", "åæ¶¦å•¤é…’", "é’å•¤", "é›€å·¢", "å¥åº·é¥®ç”¨æ°´", "æˆ˜ç•¥åˆä½œ"]
# è®°å½•å·²æ¨é€çš„æ–°é—»é“¾æ¥ï¼ˆé¿å…é‡å¤ï¼‰
pushed_links = set()

def send_to_wecom_markdown(content):
    """å‘é€å¸¦è¶…é“¾æ¥çš„Markdownæ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
    if not WEBHOOK_URL:
        print("æœªé…ç½®ä¼ä¸šå¾®ä¿¡Webhookï¼Œè·³è¿‡å‘é€")
        return
    
    data = {
        "msgtype": "markdown",
        "markdown": {
            "content": content
        }
    }
    try:
        resp = requests.post(WEBHOOK_URL, json=data, timeout=10)
        if resp.status_code == 200 and resp.json()["errcode"] == 0:
            print("Markdownæ¶ˆæ¯æ¨é€æˆåŠŸ")
        else:
            print(f"æ¨é€å¤±è´¥ï¼š{resp.text}")
    except Exception as e:
        print(f"æ¨é€å¼‚å¸¸ï¼š{str(e)}")

def is_today(date_str):
    """åˆ¤æ–­æ–°é—»å‘å¸ƒæ—¶é—´æ˜¯å¦ä¸ºä»Šå¤©"""
    try:
        # åŒ¹é… 2026-02-28 æ ¼å¼çš„æ—¥æœŸ
        news_date = datetime.strptime(date_str, "%Y-%m-%d").date()
        today = date.today()
        return news_date == today
    except:
        # åŒ¹é… 2026/02/28 æ ¼å¼çš„æ—¥æœŸ
        try:
            news_date = datetime.strptime(date_str, "%Y/%m/%d").date()
            return news_date == date.today()
        except:
            # æ— æ³•è¯†åˆ«æ—¥æœŸï¼Œé»˜è®¤ä¸æ¨é€ï¼ˆé¿å…æ—§é—»ï¼‰
            return False

def extract_industry_news():
    """æŠ“å–æŒ‡å®šè¡Œä¸šèµ„è®¯æ ç›®çš„æ–°é—»ï¼ˆæ ‡é¢˜/é“¾æ¥/å‘å¸ƒæ—¶é—´ï¼‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.beerw.com"
    }
    news_list = []
    
    try:
        resp = requests.get(TARGET_URL, headers=headers, timeout=15)
        resp.encoding = "utf-8"  # å¼ºåˆ¶UTF-8ç¼–ç ï¼Œé¿å…ä¹±ç 
        soup = BeautifulSoup(resp.text, "html.parser")

        # é€‚é…è¡Œä¸šèµ„è®¯é¡µé¢ç»“æ„ï¼šæŠ“å–æ–°é—»åˆ—è¡¨é¡¹
        news_items = soup.find_all("a", href=re.compile(r"/news/\d+\.html"))
        for item in news_items:
            # æå–æ–°é—»é“¾æ¥å’Œæ ‡é¢˜
            a_tag = item.find("a", href=re.compile(r"/news/\d+\.html"))
            if not a_tag:
                continue
            
            title = a_tag.get_text(strip=True)
            link = a_tag.get("href")
            if not title or len(title) < 5 or not link:
                continue
            
            # è¡¥å…¨å®Œæ•´é“¾æ¥
            if not link.startswith("http"):
                link = f"https://www.beerw.com{link}"
            
            # æå–å‘å¸ƒæ—¶é—´ï¼ˆé€‚é…å¤šç§æ—¶é—´æ ¼å¼ï¼‰
            publish_time = ""
            item_text = item.get_text()
            time_match = re.search(r"(\d{4}[-/]\d{2}[-/]\d{2})", item_text)
            if time_match:
                publish_time = time_match.group(1)

            news_list.append({
                "title": title,
                "link": link,
                "time": publish_time
            })
        
        return news_list
    except Exception as e:
        print(f"æŠ“å–è¡Œä¸šèµ„è®¯å¤±è´¥ï¼š{str(e)}")
        return []

def check_news_keywords(news):
    """æ£€æŸ¥æ–°é—»æ ‡é¢˜/å†…å®¹æ˜¯å¦åŒ…å«å…³é”®è¯"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        # å…ˆæ£€æŸ¥æ ‡é¢˜ï¼ˆæ ‡é¢˜å‘½ä¸­ç›´æ¥è¿”å›ï¼Œä¸ç”¨çˆ¬æ­£æ–‡ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        title_matched = [kw for kw in KEYWORDS if kw in news["title"]]
        if title_matched:
            return title_matched
        
        # æ ‡é¢˜æœªå‘½ä¸­ï¼Œå†çˆ¬æ­£æ–‡æ£€æŸ¥
        resp = requests.get(news["link"], headers=headers, timeout=10)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        content_text = soup.get_text()
        
        content_matched = [kw for kw in KEYWORDS if kw in content_text]
        return content_matched
    except Exception as e:
        print(f"æ£€æŸ¥æ–°é—» {news['link']} å¤±è´¥ï¼š{str(e)}")
        return []

def run_monitor():
    global pushed_links
    today = date.today().strftime("%Y-%m-%d")
    print(f"[{datetime.now()}] å¼€å§‹ç›‘æ§ beerw è¡Œä¸šèµ„è®¯ï¼ˆä»…æ¨é€ä»Šæ—¥æ–°é—»ï¼‰...")
    
    # 1. æŠ“å–æŒ‡å®šæ ç›®æ–°é—»
    news_list = extract_industry_news()
    if not news_list:
        print("æœªæŠ“å–åˆ°ä»»ä½•è¡Œä¸šèµ„è®¯")
        return
    
    # 2. éå†æ–°é—»ï¼Œç­›é€‰ä»Šæ—¥+æœªæ¨é€+å«å…³é”®è¯çš„æ–°é—»
    for news in news_list:
        # è·³è¿‡å·²æ¨é€çš„æ–°é—»
        if news["link"] in pushed_links:
            continue
        
        # åªå¤„ç†ä»Šå¤©å‘å¸ƒçš„æ–°é—»
       # if not news["time"] or not is_today(news["time"]):
        #    continue
       # ä¸´æ—¶æµ‹è¯•ï¼šä¸è¿‡æ»¤æ—¥æœŸ
        if not news["time"] or not is_today(news["time"]):
            continue 
        # æ£€æŸ¥å…³é”®è¯
        matched_kws = check_news_keywords(news)
        if matched_kws:
            # æ„é€ å¸¦è¶…é“¾æ¥çš„Markdownæ¶ˆæ¯
            md_content = (
    f"ğŸº **Beerw è¡Œä¸šèµ„è®¯æé†’**\n"
    f"[{news['title']}]({news['link']})\n"
    f"å‘å¸ƒæ—¶é—´ï¼š{news['time']}\n"
    f"å‘½ä¸­å…³é”®è¯ï¼š{', '.join(matched_kws)}\n"
    f"@all"
)
            print(f"æ¨é€ä»Šæ—¥æ–°é—»ï¼š{news['title']}")
            send_to_wecom_markdown(md_content)
            pushed_links.add(news["link"])
    
    # æ¸…ç†è¿‡æ—§çš„æ¨é€è®°å½•ï¼ˆåªä¿ç•™æœ€è¿‘200æ¡ï¼‰
    if len(pushed_links) > 200:
        pushed_links = set(list(pushed_links)[-200:])
    
    print(f"[{datetime.now()}] æœ¬è½®ç›‘æ§ç»“æŸï¼Œå·²æ¨é€è®°å½•æ•°ï¼š{len(pushed_links)}")

if __name__ == "__main__":
    run_monitor()
