import requests
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime, date, timedelta

# ä»ŽçŽ¯å¢ƒå˜é‡è¯»å– Webhook
WEBHOOK_URL = os.getenv("WECOM_WEBHOOK")
# ç›®æ ‡è¡Œä¸šèµ„è®¯æ ç›®
TARGET_URL = "https://www.beerw.com/class.asp?id=11"
# ç›‘æŽ§å…³é”®è¯ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
KEYWORDS = ["é’å²›å•¤é…’", "åŽæ¶¦å•¤é…’", "é’å•¤", "ç™¾å¨å•¤é…’", "å¤§éº¦", "é…’èŠ±","é…µæ¯","ç‡•äº¬å•¤é…’"]
# å·²æŽ¨é€é“¾æŽ¥ï¼ˆåŽ»é‡ï¼‰
pushed_links = set()

def send_to_wecom_markdown(content):
    """å‘é€ Markdown æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
    if not WEBHOOK_URL:
        print("âŒ æœªé…ç½® WECOM_WEBHOOK çŽ¯å¢ƒå˜é‡")
        return
    data = {
        "msgtype": "markdown",
        "markdown": {"content": content}
    }
    try:
        resp = requests.post(WEBHOOK_URL, json=data, timeout=10)
        result = resp.json()
        if result["errcode"] == 0:
            print("âœ… æ¶ˆæ¯æŽ¨é€æˆåŠŸ")
        else:
            print(f"âŒ æŽ¨é€å¤±è´¥ï¼š{result['errmsg']}")
    except Exception as e:
        print(f"âŒ æŽ¨é€å¼‚å¸¸ï¼š{str(e)}")

def is_within_7_days(date_str):
    """åˆ¤æ–­æ–°é—»æ˜¯å¦ä¸ºè¿‘7å¤©å‘å¸ƒï¼ˆæ ¸å¿ƒä¿®æ”¹ç‚¹ï¼‰"""
    if not date_str:
        return False
    # å…¼å®¹å¤šç§æ—¥æœŸæ ¼å¼
    for fmt in ["%Y-%m-%d", "%Y/%m/%d", "%Yå¹´%mæœˆ%dæ—¥"]:
        try:
            news_date = datetime.strptime(date_str, fmt).date()
            # è®¡ç®—å½“å‰æ—¥æœŸ - æ–°é—»æ—¥æœŸ â‰¤ 7å¤©
            days_diff = (date.today() - news_date).days
            return days_diff >= 0 and days_diff <= 7
        except:
            continue
    return False

def extract_industry_news():
    """æŠ“å–è¡Œä¸šèµ„è®¯æ–°é—»"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": "https://www.beerw.com",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }
    news_list = []
    
    try:
        session = requests.Session()
        session.mount('https://', requests.adapters.HTTPAdapter(max_retries=3))
        resp = session.get(TARGET_URL, headers=headers, timeout=20)
        resp.encoding = "gb2312"
        html_text = resp.text
        
        soup = BeautifulSoup(html_text, "html.parser")
        list_items = soup.find_all("li")
        print(f"ðŸ” æ‰¾åˆ° {len(list_items)} ä¸ªåˆ—è¡¨é¡¹")
        
        for li in list_items:
            a_tag = li.find("a")
            if not a_tag:
                continue
            title = a_tag.get_text(strip=True)
            link = a_tag.get("href", "")
            if not title or len(title) < 5 or not link:
                continue
            
            # è¡¥å…¨é“¾æŽ¥
            if link.startswith("/"):
                link = f"https://www.beerw.com{link}"
            elif not link.startswith("http"):
                link = f"https://www.beerw.com/{link}"
            
            # æå–å¹¶æ ¼å¼åŒ–æ—¶é—´
            publish_time = ""
            li_text = li.get_text()
            time_match = re.search(r"(\d{4}[-/å¹´]\d{2}[-/æœˆ]\d{2}æ—¥?)", li_text)
            if time_match:
                publish_time = time_match.group(1).replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            
            news_list.append({"title": title, "link": link, "time": publish_time})
        
        # åŽ»é‡
        news_list = [dict(t) for t in {tuple(d.items()) for d in news_list}]
        print(f"âœ… æœ€ç»ˆæŠ“å–åˆ° {len(news_list)} æ¡æœ‰æ•ˆæ–°é—»")
        return news_list
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ï¼š{str(e)}")
        return []

def check_news_keywords(news):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
    return [kw for kw in KEYWORDS if kw in news["title"]]

def run_monitor():
    global pushed_links
    print(f"[{datetime.now()}] å¼€å§‹ç›‘æŽ§ beerw è¡Œä¸šèµ„è®¯ï¼ˆè¿‘7å¤©ï¼‰...")
    news_list = extract_industry_news()
    
    # æ­£å¼æŽ¨é€ï¼šä»…å¤„ç†ã€è¿‘7å¤©å‘å¸ƒ + å«å…³é”®è¯ + æœªæŽ¨é€ã€‘çš„æ–°é—»
    for news in news_list:
        if news["link"] in pushed_links:
            continue
        if not news["time"] or not is_within_7_days(news["time"]):
            continue
        matched_kws = check_news_keywords(news)
        if matched_kws:
            md_content = (
                f"ðŸº **Beerw è¡Œä¸šèµ„è®¯æé†’**\n"
                f"[{news['title']}]({news['link']})\n"
                f"å‘å¸ƒæ—¶é—´ï¼š{news['time']}\n"
                f"å‘½ä¸­å…³é”®è¯ï¼š{', '.join(matched_kws)}"
            )
            print(f"ðŸ“¤ æŽ¨é€æ–°é—»ï¼š{news['title']}ï¼ˆå…³é”®è¯ï¼š{matched_kws}ï¼‰")
            send_to_wecom_markdown(md_content)
            pushed_links.add(news["link"])
    
    print("æœ¬è½®ç›‘æŽ§ç»“æŸ")

if __name__ == "__main__":
    run_monitor()
