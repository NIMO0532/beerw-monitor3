import requests
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime, date

# ä»ç¯å¢ƒå˜é‡è¯»å– Webhook
WEBHOOK_URL = os.getenv("WECOM_WEBHOOK")
# ç›®æ ‡è¡Œä¸šèµ„è®¯æ ç›®
TARGET_URL = "https://www.beerw.com/class.asp?id=11"
# ç›‘æ§å…³é”®è¯
KEYWORDS = ["é’å²›å•¤é…’", "åæ¶¦å•¤é…’", "ç™¾å¨å•¤é…’"]
# å·²æ¨é€é“¾æ¥ï¼ˆå»é‡ï¼‰
pushed_links = set()

def send_to_wecom_markdown(content):
    """å‘é€ Markdown æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
    if not WEBHOOK_URL:
        print("âŒ æœªé…ç½® WECOM_WEBHOOK ç¯å¢ƒå˜é‡")
        return
    data = {
        "msgtype": "markdown",
        "markdown": {"content": content}
    }
    try:
        resp = requests.post(WEBHOOK_URL, json=data, timeout=10)
        result = resp.json()
        if result["errcode"] == 0:
            print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸ")
        else:
            print(f"âŒ æ¨é€å¤±è´¥ï¼š{result['errmsg']}")
    except Exception as e:
        print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")

def is_today(date_str):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä»Šæ—¥æ–°é—»"""
    if not date_str:
        return False
    for fmt in ["%Y-%m-%d", "%Y/%m/%d", "%Yå¹´%mæœˆ%dæ—¥"]:
        try:
            news_date = datetime.strptime(date_str, fmt).date()
            return news_date == date.today()
        except:
            continue
    return False

def extract_industry_news():
    """æŠ“å–æ–°é—»ï¼šé€‚é…æ‰€æœ‰é“¾æ¥æ ¼å¼ï¼Œä¸é™åˆ¶/news/"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": "https://www.beerw.com",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }
    news_list = []
    
    try:
        # è¶…æ—¶é‡è¯•
        session = requests.Session()
        session.mount('https://', requests.adapters.HTTPAdapter(max_retries=3))
        resp = session.get(TARGET_URL, headers=headers, timeout=20)
        resp.encoding = "gb2312"  # å›ºå®šç½‘ç«™ç¼–ç 
        html_text = resp.text
        
        # è§£æé¡µé¢ï¼šæŠ“å–æ‰€æœ‰åˆ—è¡¨é¡¹ä¸­çš„æ ‡é¢˜é“¾æ¥ï¼ˆé€‚é…è¡Œä¸šèµ„è®¯é¡µç»“æ„ï¼‰
        soup = BeautifulSoup(html_text, "html.parser")
        
        # æ ¸å¿ƒï¼šæŠ“å–é¡µé¢ä¸­æ‰€æœ‰<li>æ ‡ç­¾é‡Œçš„<a>é“¾æ¥ï¼ˆè¡Œä¸šèµ„è®¯é¡µçš„æ–°é—»éƒ½åœ¨åˆ—è¡¨é‡Œï¼‰
        list_items = soup.find_all("li")
        print(f"ğŸ” æ‰¾åˆ° {len(list_items)} ä¸ªåˆ—è¡¨é¡¹")
        
        for li in list_items:
            a_tag = li.find("a")
            if not a_tag:
                continue
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title = a_tag.get_text(strip=True)
            link = a_tag.get("href", "")
            if not title or len(title) < 5 or not link:
                continue
            
            # è¡¥å…¨é“¾æ¥ï¼ˆå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
            if link.startswith("/"):
                link = f"https://www.beerw.com{link}"
            elif not link.startswith("http"):
                link = f"https://www.beerw.com/{link}"
            
            # æå–å‘å¸ƒæ—¶é—´ï¼ˆä»<li>æ–‡æœ¬ä¸­æ‰¾æ—¥æœŸï¼‰
            publish_time = ""
            li_text = li.get_text()
            time_match = re.search(r"(\d{4}[-/å¹´]\d{2}[-/æœˆ]\d{2}æ—¥?)", li_text)
            if time_match:
                publish_time = time_match.group(1).replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            
            news_list.append({
                "title": title,
                "link": link,
                "time": publish_time
            })
        
        # å»é‡ï¼ˆé¿å…é‡å¤é“¾æ¥ï¼‰
        news_list = [dict(t) for t in {tuple(d.items()) for d in news_list}]
        print(f"âœ… æœ€ç»ˆæŠ“å–åˆ° {len(news_list)} æ¡æœ‰æ•ˆæ–°é—»")
        return news_list
    
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ï¼š{str(e)}")
        return []

def check_news_keywords(news):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
    matched = [kw for kw in KEYWORDS if kw in news["title"]]
    if matched:
        print(f"ğŸ” æ–°é—» {news['title']} å‘½ä¸­å…³é”®è¯ï¼š{matched}")
    return matched

def run_monitor():
    global pushed_links
    print(f"[{datetime.now()}] å¼€å§‹ç›‘æ§ beerw è¡Œä¸šèµ„è®¯...")
    news_list = extract_industry_news()
    
    # 1. æµ‹è¯•æ¨é€ï¼šå¼ºåˆ¶æ¨é€ç¬¬ä¸€æ¡æ–°é—»ï¼ˆä¸ç®¡æ—¥æœŸ/å…³é”®è¯ï¼‰
    if news_list:
        test_news = news_list[0]
        md_content = (
            f"ğŸº **Beerw ç›‘æ§æé†’ï¼ˆæµ‹è¯•ï¼‰**\n"
            f"[{test_news['title']}]({test_news['link']})\n"
            f"å‘å¸ƒæ—¶é—´ï¼š{test_news['time'] or 'æœªçŸ¥'}\n"
            f"æµ‹è¯•è¯´æ˜ï¼šå¼ºåˆ¶æ¨é€ç¬¬ä¸€æ¡æ–°é—»éªŒè¯æŠ“å–åŠŸèƒ½"
        )
        print(f"ğŸ“¤ æ¨é€æµ‹è¯•æ–°é—»ï¼š{test_news['title']}")
        send_to_wecom_markdown(md_content)
        pushed_links.add(test_news["link"])
    else:
        print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»ï¼Œå‘é€æµ‹è¯•æ¶ˆæ¯...")
        send_to_wecom_markdown("ğŸº **Beerw ç›‘æ§æµ‹è¯•**\né€šé“æ­£å¸¸ï¼Œä½†æœªæŠ“å–åˆ°æ–°é—»åˆ—è¡¨")
    
    # 2. æ­£å¼æ¨é€ï¼šä»Šæ—¥+å«å…³é”®è¯çš„æ–°é—»ï¼ˆæµ‹è¯•å®Œæˆåå¯å–æ¶ˆæ³¨é‡Šï¼‰
     for news in news_list:
         if news["link"] in pushed_links:
             continue
         if not news["time"] or not is_today(news["time"]):
             continue
         matched_kws = check_news_keywords(news)
         if matched_kws:
             md_content = (
                 f"ğŸº **Beerw è¡Œä¸šèµ„è®¯æé†’**\n"
                 f"[{news['title']}]({news['link']})\n"
                 f"å‘å¸ƒæ—¶é—´ï¼š{news['time']}\n"
                 f"å‘½ä¸­å…³é”®è¯ï¼š{', '.join(matched_kws)}"
             )
             send_to_wecom_markdown(md_content)
             pushed_links.add(news["link"])
    
    print("æœ¬è½®ç›‘æ§ç»“æŸ")

if __name__ == "__main__":
    run_monitor()
