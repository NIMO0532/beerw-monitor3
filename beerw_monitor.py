import requests
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime

# ä» GitHub Actions ç¯å¢ƒå˜é‡è¯»å– Webhook
WEBHOOK_URL = os.getenv("WECOM_WEBHOOK")
TARGET_URL = "https://www.beerw.com"
# ä½ è¦ç›‘æ§çš„å…³é”®è¯ï¼ˆå¯ä¿®æ”¹ï¼‰
KEYWORDS = ["é’å²›å•¤é…’", "åæ¶¦å•¤é…’", "é’å•¤", "é›€å·¢", "å¥åº·é¥®ç”¨æ°´", "æˆ˜ç•¥åˆä½œ"]

# è®°å½•å·²ç»æ¨é€è¿‡çš„æ–°é—»é“¾æ¥ï¼Œé¿å…é‡å¤æé†’
pushed_links = set()

def send_to_wecom_markdown(content):
    """å‘é€ Markdown æ ¼å¼æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆæ”¯æŒè¶…é“¾æ¥ï¼‰"""
    if not WEBHOOK_URL:
        print("æœªé…ç½® Webhookï¼Œè·³è¿‡å‘é€")
        return
    data = {
        "msgtype": "markdown",
        "markdown": {
            "content": content
        }
    }
    try:
        resp = requests.post(WEBHOOK_URL, json=data, timeout=10)
        if resp.status_code == 200:
            print("Markdown æ¶ˆæ¯å‘é€æˆåŠŸ")
        else:
            print(f"å‘é€å¤±è´¥ï¼š{resp.text}")
    except Exception as e:
        print(f"å‘é€å¼‚å¸¸ï¼š{str(e)}")

def extract_news_list():
    """ä» beerw.com é¦–é¡µæå–æ–°é—»åˆ—è¡¨ï¼ˆæ ‡é¢˜ã€é“¾æ¥ã€æ—¶é—´ï¼‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.beerw.com"
    }
    news_list = []
    try:
        resp = requests.get(TARGET_URL, headers=headers, timeout=15)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")

        # é€‚é… beerw.com ç»“æ„ï¼šæŠ“å–æ‰€æœ‰å¸¦ /news/ çš„é“¾æ¥
        for a_tag in soup.find_all("a", href=re.compile(r"/news/\d+\.html")):
            title = a_tag.get_text(strip=True)
            link = a_tag.get("href")
            if not title or len(title) < 5 or not link:
                continue

            # è¡¥å…¨ä¸ºå®Œæ•´é“¾æ¥
            if not link.startswith("http"):
                link = f"https://www.beerw.com{link}"

            # å°è¯•æå–å‘å¸ƒæ—¶é—´ï¼ˆä»çˆ¶çº§æˆ–ç›¸é‚»å…ƒç´ æ‰¾ï¼‰
            publish_time = ""
            parent = a_tag.parent
            if parent:
                # æ‰¾åŒ…å«æ—¥æœŸæ ¼å¼çš„æ–‡æœ¬ï¼Œå¦‚ 2026-02-26
                time_match = re.search(r"(\d{4}-\d{2}-\d{2})", parent.get_text())
                if time_match:
                    publish_time = time_match.group(1)

            news_list.append({
                "title": title,
                "link": link,
                "time": publish_time or "æœªçŸ¥æ—¶é—´"
            })
        return news_list
    except Exception as e:
        print(f"æŠ“å–æ–°é—»åˆ—è¡¨å¼‚å¸¸ï¼š{str(e)}")
        return []

def check_news_keywords(news):
    """æ£€æŸ¥å•ç¯‡æ–°é—»æ˜¯å¦åŒ…å«å…³é”®è¯"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        resp = requests.get(news["link"], headers=headers, timeout=10)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        content_text = soup.get_text()

        matched = [kw for kw in KEYWORDS if kw in news["title"] or kw in content_text]
        return matched
    except Exception as e:
        print(f"æ£€æŸ¥æ–°é—» {news['link']} å¼‚å¸¸ï¼š{str(e)}")
        return []

def run_monitor():
    global pushed_links
    print(f"[{datetime.now()}] å¼€å§‹ç›‘æ§ beerw.com...")
    news_list = extract_news_list()
    if not news_list:
        print("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")
        return

    for news in news_list:
        if news["link"] in pushed_links:
            continue

        matched_kws = check_news_keywords(news)
        if matched_kws:
            # æ„é€  Markdown æ¶ˆæ¯ï¼šå¸¦è¶…é“¾æ¥æ ‡é¢˜ + æ—¶é—´ + å…³é”®è¯
            md_content = (
                f"ğŸº **Beerw ç›‘æ§æé†’**\n\n"
                f"**[{news['title']}]({news['link']})**\n\n"
                f"å‘å¸ƒæ—¶é—´ï¼š{news['time']}\n\n"
                f"å‘½ä¸­å…³é”®è¯ï¼š{', '.join(matched_kws)}\n\n"
                f"@all"
            )
            print(f"å‘ç°æ–°é—»ï¼š{news['title']}ï¼Œæ¨é€ä¸­...")
            send_to_wecom_markdown(md_content)
            pushed_links.add(news["link"])

    # é™åˆ¶å·²æ¨é€è®°å½•æ•°é‡ï¼Œé˜²æ­¢å†…å­˜è¿‡å¤§
    if len(pushed_links) > 500:
        pushed_links = set(list(pushed_links)[-200:])
    print("æœ¬è½®ç›‘æ§ç»“æŸ")

if __name__ == "__main__":
    run_monitor()
