import requests
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime, date

# ä»ç¯å¢ƒå˜é‡è¯»å– Webhook
WEBHOOK_URL = os.getenv("WECOM_WEBHOOK")
# ç›®æ ‡è¡Œä¸šèµ„è®¯æ ç›®
TARGET_URL = "https://www.beerw.com/class.asp?id=11"
# ç›‘æ§å…³é”®è¯
KEYWORDS = ["é’å²›å•¤é…’", "åæ¶¦å•¤é…’", "ç™¾å¨å•¤é…’", "é›€å·¢",]
# å·²æ¨é€é“¾æ¥ï¼ˆå»é‡ï¼‰
pushed_links = set()

def send_to_wecom_markdown(content):
    """å‘é€ Markdown æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆç®€åŒ–æ ¼å¼ï¼Œé¿å…å…¼å®¹é—®é¢˜ï¼‰"""
    if not WEBHOOK_URL:
        print("âŒ æœªé…ç½® WECOM_WEBHOOK ç¯å¢ƒå˜é‡")
        return
    data = {
        "msgtype": "markdown",
        "markdown": {"content": content}
    }
    try:
        resp = requests.post(WEBHOOK_URL, json=data, timeout=10)
        result = resp.json()
        if result["errcode"] == 0:
            print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸ")
        else:
            print(f"âŒ æ¨é€å¤±è´¥ï¼š{result['errmsg']}")
    except Exception as e:
        print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")

def is_today(date_str):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä»Šæ—¥æ–°é—»ï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰"""
    if not date_str:
        return False
    for fmt in ["%Y-%m-%d", "%Y/%m/%d", "%Yå¹´%mæœˆ%dæ—¥"]:
        try:
            news_date = datetime.strptime(date_str, fmt).date()
            return news_date == date.today()
        except:
            continue
    return False

def extract_industry_news():
    """æŠ“å–æ–°é—»ï¼ˆå¢åŠ ååçˆ¬+è°ƒè¯•æ—¥å¿—ï¼‰"""
    # å¢å¼ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": "https://www.beerw.com",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache"
    }
    news_list = []
    
    try:
        # å¢åŠ è¶…æ—¶é‡è¯•
        session = requests.Session()
        session.mount('https://', requests.adapters.HTTPAdapter(max_retries=3))
        resp = session.get(TARGET_URL, headers=headers, timeout=20)
        
        # è°ƒè¯•ï¼šè¾“å‡ºé¡µé¢çŠ¶æ€å’Œç¼–ç 
        print(f"ğŸ” é¡µé¢å“åº”çŠ¶æ€ç ï¼š{resp.status_code}")
        print(f"ğŸ” ç½‘ç«™è‡ªåŠ¨è¯†åˆ«ç¼–ç ï¼š{resp.apparent_encoding}")
        
        # å¼ºåˆ¶å°è¯•å¤šç§ç¼–ç è§£æ
        encodings = ["gb2312", "gbk", "utf-8", "iso-8859-1"]
        html_text = ""
        for enc in encodings:
            try:
                resp.encoding = enc
                html_text = resp.text
                if html_text:
                    print(f"âœ… ä½¿ç”¨ç¼–ç  {enc} è§£ææˆåŠŸ")
                    break
            except:
                continue
        
        if not html_text:
            print("âŒ æ‰€æœ‰ç¼–ç è§£æå‡å¤±è´¥")
            return []
        
        # è°ƒè¯•ï¼šè¾“å‡ºå‰500å­—ç¬¦ï¼Œç¡®è®¤æ˜¯å¦æŠ“åˆ°é¡µé¢å†…å®¹
        print(f"ğŸ” é¡µé¢å‰500å­—ç¬¦ï¼š{html_text[:500]}")
        
        # è§£æé¡µé¢ï¼ˆæ”¾å®½åŒ¹é…æ¡ä»¶ï¼‰
        soup = BeautifulSoup(html_text, "html.parser")
        # åŒ¹é…æ‰€æœ‰åŒ…å« news çš„é“¾æ¥ï¼ˆä¸ç®¡æ ¼å¼ï¼‰
        all_links = soup.find_all("a")
        news_links = []
        for a in all_links:
            href = a.get("href", "")
            if "/news/" in href and ".html" in href:
                news_links.append(a)
        
        print(f"ğŸ” æ‰¾åˆ° {len(news_links)} æ¡æ–°é—»é“¾æ¥")
        
        # æå–æ–°é—»ä¿¡æ¯
        for a_tag in news_links:
            title = a_tag.get_text(strip=True)
            link = a_tag.get("href")
            if not title or len(title) < 2:
                continue
            # è¡¥å…¨é“¾æ¥
            if not link.startswith("http"):
                link = f"https://www.beerw.com{link}"
            # æå–æ—¶é—´ï¼ˆä»ç›¸é‚»æ–‡æœ¬æ‰¾ï¼‰
            publish_time = ""
            time_match = re.search(r"(\d{4}[-/å¹´]\d{2}[-/æœˆ]\d{2}æ—¥?)", a_tag.parent.get_text())
            if time_match:
                publish_time = time_match.group(1).replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            
            news_list.append({"title": title, "link": link, "time": publish_time})
        
        print(f"âœ… æœ€ç»ˆæŠ“å–åˆ° {len(news_list)} æ¡æœ‰æ•ˆæ–°é—»")
        return news_list
    
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ï¼š{str(e)}")
        return []

def check_news_keywords(news):
    """æ£€æŸ¥å…³é”®è¯ï¼ˆä»…æ£€æŸ¥æ ‡é¢˜ï¼Œé¿å…äºŒæ¬¡è®¿é—®è¢«å±è”½ï¼‰"""
    matched = [kw for kw in KEYWORDS if kw in news["title"]]
    if matched:
        print(f"ğŸ” æ–°é—» {news['title']} å‘½ä¸­å…³é”®è¯ï¼š{matched}")
    return matched

def run_monitor():
    global pushed_links
    print(f"[{datetime.now()}] å¼€å§‹ç›‘æ§ beerw è¡Œä¸šèµ„è®¯...")
    news_list = extract_industry_news()
    
    # ä¸´æ—¶æµ‹è¯•ï¼šå¼ºåˆ¶æ¨é€ç¬¬ä¸€æ¡æ–°é—»ï¼ˆä¸ç®¡æ—¥æœŸ/å…³é”®è¯ï¼‰
    if news_list:
        test_news = news_list[0]
        md_content = (
            f"ğŸº **Beerw æµ‹è¯•æé†’**\n"
            f"[{test_news['title']}]({test_news['link']})\n"
            f"å‘å¸ƒæ—¶é—´ï¼š{test_news['time']}\n"
            f"æµ‹è¯•æ¨é€ï¼šå¼ºåˆ¶å‘é€ç¬¬ä¸€æ¡æ–°é—»éªŒè¯é€šé“"
        )
        print(f"ğŸ“¤ å¼ºåˆ¶æ¨é€æµ‹è¯•æ–°é—»ï¼š{test_news['title']}")
        send_to_wecom_markdown(md_content)
        pushed_links.add(test_news["link"])
    else:
        print("âŒ æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»ï¼Œå‘é€æµ‹è¯•æ¶ˆæ¯...")
        # å³ä½¿æ²¡æŠ“åˆ°æ–°é—»ï¼Œä¹Ÿå‘ä¸€æ¡æµ‹è¯•æ¶ˆæ¯éªŒè¯æ¨é€é€šé“
        send_to_wecom_markdown("ğŸº **Beerw ç›‘æ§æµ‹è¯•**\né€šé“æ­£å¸¸ï¼Œä½†æœªæŠ“å–åˆ°æ–°é—»ï¼ˆå¯èƒ½è¢«ç½‘ç«™å±è”½ï¼‰")
    
    print("æœ¬è½®ç›‘æ§ç»“æŸ")

if __name__ == "__main__":
    run_monitor()
