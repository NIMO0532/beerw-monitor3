import requests
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime, date, timedelta

# ä»ç¯å¢ƒå˜é‡è¯»å– Webhook
WEBHOOK_URL = os.getenv("WECOM_WEBHOOK")
# ç›®æ ‡è¡Œä¸šèµ„è®¯æ ç›®
TARGET_URL = "https://www.beerw.com/class.asp?id=11"
# ç›‘æ§å…³é”®è¯ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
KEYWORDS = ["é’å²›å•¤é…’", "åæ¶¦å•¤é…’", "é’å•¤", "ç™¾å¨å•¤é…’", "å¤§éº¦", "é…’èŠ±","é…µæ¯","ç‡•äº¬å•¤é…’", "å•¤é…’"]
# å·²æ¨é€é“¾æ¥ï¼ˆå»é‡ï¼‰
pushed_links = set()

def send_to_wecom_markdown(content):
    """å‘é€ Markdown æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
    if not WEBHOOK_URL:
        print("âŒ æœªé…ç½® WECOM_WEBHOOK ç¯å¢ƒå˜é‡")
        return
    data = {
        "msgtype": "markdown",
        "markdown": {"content": content}
    }
    try:
        resp = requests.post(WEBHOOK_URL, json=data, timeout=10)
        result = resp.json()
        if result["errcode"] == 0:
            print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸ")
        else:
            print(f"âŒ æ¨é€å¤±è´¥ï¼š{result['errmsg']}")
    except Exception as e:
        print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")

def is_within_7_days(date_str):
    """åˆ¤æ–­æ–°é—»æ˜¯å¦ä¸ºè¿‘7å¤©å‘å¸ƒ"""
    if not date_str:
        return False
    # å…¼å®¹å¤šç§æ—¥æœŸæ ¼å¼
    for fmt in ["%Y-%m-%d", "%Y/%m/%d", "%Yå¹´%mæœˆ%dæ—¥"]:
        try:
            news_date = datetime.strptime(date_str, fmt).date()
            # è®¡ç®—å½“å‰æ—¥æœŸ - æ–°é—»æ—¥æœŸ â‰¤ 7å¤©
            days_diff = (date.today() - news_date).days
            return days_diff >= 0 and days_diff <= 7
        except:
            continue
    return False

def extract_industry_news():
    """æŠ“å–è¡Œä¸šèµ„è®¯æ–°é—»"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": "https://www.beerw.com",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }
    news_list = []

    try:
        session = requests.Session()
        session.mount('https://', requests.adapters.HTTPAdapter(max_retries=3))
        resp = session.get(TARGET_URL, headers=headers, timeout=20)
        resp.encoding = "gb2312"
        html_text = resp.text

        soup = BeautifulSoup(html_text, "html.parser")
        list_items = soup.find_all("li")
        print(f"ğŸ” æ‰¾åˆ° {len(list_items)} ä¸ªåˆ—è¡¨é¡¹")

        for li in list_items:
            a_tag = li.find("a")
            if not a_tag:
                continue
            title = a_tag.get_text(strip=True)
            link = a_tag.get("href", "")
            if not title or len(title) < 5 or not link:
                continue

            # è¡¥å…¨é“¾æ¥
            if link.startswith("/"):
                link = f"https://www.beerw.com{link}"
            elif not link.startswith("http"):
                link = f"https://www.beerw.com/{link}"

            # æå–å¹¶æ ¼å¼åŒ–æ—¶é—´
            publish_time = ""
            li_text = li.get_text()
            time_match = re.search(r"(\d{4}[-/å¹´]\d{2}[-/æœˆ]\d{2}æ—¥?)", li_text)
            if time_match:
                publish_time = time_match.group(1).replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")

            news_list.append({"title": title, "link": link, "time": publish_time})

        # å»é‡
        news_list = [dict(t) for t in {tuple(d.items()) for d in news_list}]
        print(f"âœ… æœ€ç»ˆæŠ“å–åˆ° {len(news_list)} æ¡æœ‰æ•ˆæ–°é—»")
        return news_list
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ï¼š{str(e)}")
        return []

def check_news_keywords(news):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
    return [kw for kw in KEYWORDS if kw in news["title"]]

def run_monitor():
    global pushed_links
    print(f"[{datetime.now()}] å¼€å§‹ç›‘æ§ beerw è¡Œä¸šèµ„è®¯ï¼ˆè¿‘7å¤©ï¼‰...")
    news_list = extract_industry_news()

    # --- ä¿®æ”¹éƒ¨åˆ†å¼€å§‹ ---
    # ç”¨äºå­˜æ”¾æœ¬è½®è¦æ¨é€çš„æ–°é—»
    pending_news = []

    for news in news_list:
        if news["link"] in pushed_links:
            continue
        if not news["time"] or not is_within_7_days(news["time"]):
            continue
        matched_kws = check_news_keywords(news)
        if matched_kws:
            # å°†ç¬¦åˆæ¡ä»¶çš„æ–°é—»åŠ å…¥åˆ—è¡¨ï¼Œä¸ç«‹å³å‘é€
            pending_news.append({
                "title": news['title'],
                "link": news['link'],
                "time": news['time'],
                "keywords": ', '.join(matched_kws)
            })
            # è®°å½•å·²å¤„ç†ï¼Œé˜²æ­¢ä¸‹æ¬¡é‡å¤æ¨é€
            pushed_links.add(news["link"])

    # å¦‚æœæœ‰æ–°é—»éœ€è¦æ¨é€ï¼Œç”Ÿæˆæ±‡æ€»å†…å®¹
    if pending_news:
        # ç”Ÿæˆ Markdown æ±‡æ€»å†…å®¹
        # é¡¶éƒ¨æ ‡é¢˜å’Œæ—¶é—´
        md_content = f"### ğŸº Beerw è¡Œä¸šèµ„è®¯æ±‡æ€»\n"
        md_content += f"> **ç›‘æ§æ—¶é—´**ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        md_content += f"> **ç­›é€‰æ¡ä»¶**ï¼šè¿‘7å¤©å‘å¸ƒï¼ŒåŒ…å«å…³é”®è¯\n\n"

        # å¾ªç¯æ‹¼æ¥æ¯æ¡æ–°é—»
        for idx, item in enumerate(pending_news, 1):
            md_content += f"**{idx}. [{item['title']}]({item['link']})**\n"
            md_content += f"    - å‘å¸ƒæ—¶é—´ï¼š{item['time']}\n"
            md_content += f"    - å‘½ä¸­å…³é”®è¯ï¼š{item['keywords']}\n\n"

        print(f"ğŸ“¤ å…±æ•´ç†åˆ° {len(pending_news)} æ¡æ–°é—»ï¼Œå‡†å¤‡åˆå¹¶å‘é€...")
        send_to_wecom_markdown(md_content)
    else:
        print("æœ¬è½®ç›‘æ§æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„æ–°æ¶ˆæ¯")

    print("æœ¬è½®ç›‘æ§ç»“æŸ")

if __name__ == "__main__":
    run_monitor()
